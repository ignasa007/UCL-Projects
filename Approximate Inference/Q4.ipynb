{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as  np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from genimages import genimages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FreeEnergy(X, sigma_sq, pie, alpha, lamda, mu, precisions, eps=1e-8):\n",
    "\n",
    "    '''\n",
    "    Free Energy wrt the variational distribution and the current estimate of \n",
    "    the parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        D x N data matrix\n",
    "    sigma_sq: float\n",
    "        noise variance\n",
    "    pie: np.ndarray\n",
    "        K x 1 vector of priors on the latents\n",
    "    alpha: np.array\n",
    "        K dimensional parameter vector of precisions of latents\n",
    "    lamda: np.ndarray\n",
    "        K x N matrix of parameter values characterizing the variational\n",
    "        distribution\n",
    "    mu: np.ndarray\n",
    "        D x K matrix of means of latent features\n",
    "    precisions: np.array\n",
    "        K dimensional vector of precisions of posterior on latents\n",
    "    eps: float\n",
    "        correction for numerical stability of log\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    F: int\n",
    "        variational free energy wrt the factored distribution and the current \n",
    "        estimate of the parameters \n",
    "    '''\n",
    "\n",
    "    N, D = X.shape[1], X.shape[0]\n",
    "    lamda_sum_n = np.sum(lamda, axis=1)\n",
    "    ESS_lamda = lamda@lamda.T; np.fill_diagonal(ESS_lamda, lamda_sum_n)\n",
    "    ESS_mu =  mu.T@mu; np.fill_diagonal(ESS_mu, np.diag(ESS_mu) + D/precisions)\n",
    "    \n",
    "    expectation_log_joint = -N*D/2 * np.log(2*np.pi*sigma_sq) + \\\n",
    "        -1/(2*sigma_sq) * (np.vdot(X, X) + np.vdot(ESS_lamda, ESS_mu) - 2*np.vdot(lamda, mu.T@X)) + \\\n",
    "        np.dot(lamda_sum_n, np.log(pie+eps)) + np.dot(N-lamda_sum_n, np.log(1-pie+eps)) + \\\n",
    "        D/2 * np.sum(np.log(2*np.pi*alpha)) - alpha@np.diag(ESS_mu)/2\n",
    "    \n",
    "    H_mBernoulli = - np.sum(lamda*np.log(lamda+eps) + (1-lamda)*np.log(1-lamda+eps)) \n",
    "    H_mGaussian = D/2 * np.sum(np.log(2*np.pi*np.e/precisions))\n",
    "    \n",
    "    print(expectation_log_joint, H_mBernoulli, H_mGaussian)\n",
    "    F = expectation_log_joint +  H_mBernoulli + H_mGaussian\n",
    "\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MeanField(X, sigma_sq, pie, alpha, lamda, mu, precisions, maxsteps, eps=1e-1):\n",
    "\n",
    "    '''\n",
    "    Factored Variational E-step for learning with Binary Latent Factor Model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        D x N data matrix\n",
    "    sigma_sq: float\n",
    "        noise variance\n",
    "    pie: np.ndarray\n",
    "        K x 1 vector of priors on the latents\n",
    "    alpha: np.array\n",
    "        K dimensional parameter vector of precisions of latents\n",
    "    lamda: np.ndarray\n",
    "        K x N matrix of initial parameter values characterizing the variational\n",
    "        distribution\n",
    "    mu: np.ndarray\n",
    "        D x K matrix of means of latent features\n",
    "    precisions: np.array\n",
    "        K dimensional vector of precisions of posterior on latents\n",
    "    maxsteps: int\n",
    "        maximum number of steps of the fixed point interations\n",
    "    eps: float\n",
    "        minimum improvement in free energy to continue iterations \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lamda: np.ndarray\n",
    "        K x N matrix of updated parameter values characterizing the variational\n",
    "        distribution\n",
    "    mu: np.ndarray\n",
    "        D x K matrix of means of latent features\n",
    "    precisions: np.array\n",
    "        precisions of distributions on latent features\n",
    "    F: float\n",
    "        Variational Free Energy\n",
    "    '''\n",
    "\n",
    "    prior_odds = (1-pie) / (pie+1e-8)\n",
    "    F = float('-inf')\n",
    "\n",
    "    for _ in range(maxsteps):\n",
    "        for i in range(pie.size):\n",
    "            mu_i, lamda_i = mu[:, [i]], lamda[[i]]\n",
    "            exponent = mu_i.T @ (-2*X + mu_i + 2*(mu@lamda-mu_i@lamda_i)) / (2*sigma_sq)\n",
    "            lamda[[i]] = 1 / (1 + prior_odds[i]*np.exp(exponent)); lamda_i = lamda[[i]]\n",
    "            precisions[i] = precisions[i] + np.sum(lamda_i)/sigma_sq\n",
    "            mu[:, [i]] = (X - (mu@lamda-mu_i@lamda_i)) @ lamda_i.T / (precisions[i]*sigma_sq)\n",
    "        old_F, F = F, FreeEnergy(X, sigma_sq, pie, alpha, lamda, mu, precisions)\n",
    "        if F-old_F < eps:\n",
    "            break\n",
    "\n",
    "    return lamda, mu, precisions, F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MStep(X, lamda, mu, precisions):\n",
    "\n",
    "    '''\n",
    "    Maximisation step for learning with Binary Latent Factor Model.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        D x N data matrix\n",
    "    lamda: np.ndarray\n",
    "        K x N matrix of parameter values characterizing the variational \n",
    "        distribution\n",
    "    mu: np.ndarray\n",
    "        D x K matrix of means of latent features\n",
    "    precisions: np.array\n",
    "        K dimensional vector of precisions of posterior on latents\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sigma_sq: float\n",
    "        updated estimate of noise variance\n",
    "    pie: np.ndarray\n",
    "        K x 1 vector of parameters characterising the distribution on \n",
    "        the latents\n",
    "    alpha: np.array\n",
    "        K dimensional parameter vector of precisions of latents\n",
    "    '''\n",
    "\n",
    "    D, N = X.shape\n",
    "    ESS_lamda = lamda@lamda.T; np.fill_diagonal(ESS_lamda, np.sum(lamda, axis=1))\n",
    "    ESS_mu =  mu.T@mu; np.fill_diagonal(ESS_mu, np.diag(ESS_mu) + D/precisions)\n",
    "    \n",
    "    sigma_sq = (np.vdot(X, X) + np.vdot(ESS_lamda, ESS_mu) - 2*np.vdot(lamda, mu.T@X)) / (N*D)\n",
    "    pie = np.mean(lamda, axis=1)\n",
    "    alpha = D / (D*precisions + np.diag(ESS_mu))\n",
    "\n",
    "    return sigma_sq, pie, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LearnBinFactors(X, K, iterations):\n",
    "\n",
    "    '''\n",
    "    Factored Variational EM for learning with Binary Latent Factor Model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        D x N data matrix\n",
    "    K: int\n",
    "        number of latent binary factors\n",
    "    iterations: int\n",
    "        maximum number of iterations of EM\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mu: np.ndarray\n",
    "        D x K matrix of means: mu @ S = X\n",
    "    sigma_sq: float\n",
    "        estimate of noise variance\n",
    "    pie: np.ndarray\n",
    "        K x 1 vector of parameters characterising the distribution on the\n",
    "        latents\n",
    "    Fs: np.ndarray\n",
    "        Free energy values after each iteration\n",
    "    '''\n",
    "\n",
    "    D, N = X.shape\n",
    "    sigma_sq = np.random.rand()*1.5-0.5; pie = np.random.rand(K); alpha = np.random.rand(K)\n",
    "    lamda = np.random.rand(K, N); mu = np.random.rand(D, K); precisions = np.random.rand(K)\n",
    "\n",
    "    Fs = np.zeros(iterations); Fs[0] = FreeEnergy(X, sigma_sq, pie, alpha, lamda, mu, precisions)\n",
    "    for i in range(iterations):\n",
    "        lamda, mu, precisions, _ = MeanField(X, sigma_sq, pie, alpha, lamda, mu, precisions, maxsteps=50, eps=0.)\n",
    "        sigma_sq, pie, alpha = MStep(X, lamda, mu, precisions)\n",
    "        Fs[i] = FreeEnergy(X, sigma_sq, pie, alpha, lamda, mu, precisions)\n",
    "        if i != 0:\n",
    "            fe_increment = Fs[i] - Fs[i-1]\n",
    "            assert fe_increment >= 0., fe_increment\n",
    "            if fe_increment < 1e-2:\n",
    "                break\n",
    "            \n",
    "    return mu, alpha, sigma_sq, pie, Fs[:i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-925106.3854318497 1593.7720751934407 292.14841255348574\n",
      "-9166.23231517059 141.4672753360619 -166.65715661371075\n",
      "-7694.6724940002105 189.75102284589013 -245.74956757420114\n",
      "-6816.454371981954 462.8899022821786 -305.42501113203775\n",
      "-6794.801599539534 776.57355589918 -344.0812437965272\n",
      "-7296.428422207424 844.1978398118825 -370.9618147130809\n",
      "-6937.977537985441 844.1978398118825 -370.9618147130809\n",
      "-8214.176287532075 1072.6167549759016 -383.67586017070914\n",
      "-8516.038054239003 1183.6479053697492 -394.5178501637479\n",
      "-8035.568803198284 1183.6479053697492 -394.5178501637479\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "-781.6972351056429",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m D \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m----> 6\u001b[0m mu, alpha, sigma_sq, pie, Fs \u001b[38;5;241m=\u001b[39m \u001b[43mLearnBinFactors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(Fs, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEM Iterations\u001b[39m\u001b[38;5;124m'\u001b[39m, font\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserif\u001b[39m\u001b[38;5;124m'\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n",
      "Cell \u001b[1;32mIn[62], line 39\u001b[0m, in \u001b[0;36mLearnBinFactors\u001b[1;34m(X, K, iterations)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     38\u001b[0m     fe_increment \u001b[38;5;241m=\u001b[39m Fs[i] \u001b[38;5;241m-\u001b[39m Fs[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m fe_increment \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m, fe_increment\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fe_increment \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-2\u001b[39m:\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: -781.6972351056429"
     ]
    }
   ],
   "source": [
    "N = 400\n",
    "X = genimages(N).T\n",
    "D = X.shape[0]\n",
    "\n",
    "K = 8\n",
    "mu, alpha, sigma_sq, pie, Fs = LearnBinFactors(X, K, 100)\n",
    "\n",
    "plt.plot(Fs, color='blue')\n",
    "plt.xlabel('EM Iterations', font='serif', size=12)\n",
    "plt.ylabel('Free Eenrgy', font='serif', size=12)\n",
    "plt.title('Factored Variational EM', font='serif', size=14)\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(8, 4.5))\n",
    "fig.patch.set_facecolor('lightblue')\n",
    "for k in range(K):\n",
    "    plt.subplot(2, 4, k+1)\n",
    "    plt.imshow(np.reshape(mu[:,k], (4, 4)), cmap=plt.gray(), interpolation='none')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
