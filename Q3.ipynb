{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM for Binary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=6, linewidth=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = np.loadtxt('binarydigits.txt')\n",
    "N, D = dataset.shape\n",
    "\n",
    "N, D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $N = 100$ images, each with $D = 64$ pixels laid out as a vector, i.e., $\\mathbf{x}^{\\left(n\\right)} \\in \\mathbb{R}^D,\\ \\forall n\\in \\left\\{1,\\ldots,N\\right\\}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMForMultivariateBernoulli:\n",
    "\n",
    "    def log_likelihood(self, X, parameters, pi):\n",
    "\n",
    "        odds = np.divide(np.minimum(parameters, 1-1e-10), 1-np.minimum(parameters, 1-1e-10))\n",
    "        marginals = list()\n",
    "        for x in X:\n",
    "            likelihoods = np.prod(np.multiply(1-parameters, np.power(odds, x)), axis=1)\n",
    "            joints = np.multiply(pi, likelihoods)\n",
    "            marginal = np.sum(joints)\n",
    "            marginals.append(marginal)\n",
    "        marginals = np.array(marginals)\n",
    "        log_likelihood = np.sum(np.log(marginals))\n",
    "\n",
    "        return log_likelihood\n",
    "\n",
    "    def expectation(self, X, parameters, priors):\n",
    "\n",
    "        odds = np.divide(np.minimum(parameters, 1-1e-10), 1-np.minimum(parameters, 1-1e-10))\n",
    "        responsibilities = list()\n",
    "        for x in X:\n",
    "            likelihoods = np.prod(np.multiply(1-parameters, np.power(odds, x)), axis=1)\n",
    "            joints = np.multiply(priors, likelihoods)\n",
    "            marginal = np.sum(joints)\n",
    "            posteriors = joints / marginal\n",
    "            responsibilities.append(posteriors)\n",
    "        responsibilities = np.array(responsibilities)\n",
    "        \n",
    "        return responsibilities\n",
    "\n",
    "    def maximisation(self, X, responsibilities):\n",
    "\n",
    "        P = np.divide(responsibilities.T @ X, np.sum(responsibilities, axis=0).reshape(-1, 1))\n",
    "        pi = np.mean(responsibilities, axis=0)\n",
    "\n",
    "        return P, pi\n",
    "\n",
    "    def __call__(self, K, X, n_iterations, epsilon):\n",
    "\n",
    "        P = np.random.uniform(size=(K, X.shape[1]))\n",
    "        pi = np.ones(K) / K\n",
    "\n",
    "        log_ls = [self.log_likelihood(X, P, pi)]\n",
    "\n",
    "        for i in range(n_iterations):\n",
    "            responsibilities = self.expectation(X=X, parameters=P, priors=pi)\n",
    "            P, pi = self.maximisation(X, responsibilities=responsibilities)\n",
    "            log_l = self.log_likelihood(X, parameters=P, pi=pi)\n",
    "            log_ls.append(log_l)\n",
    "            if log_ls[-1] - log_ls[-2] <= epsilon:\n",
    "                break\n",
    "\n",
    "        return P, pi, responsibilities, log_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(p, eps=1e-8):\n",
    "\n",
    "    nonzero = np.nonzero(p)\n",
    "    if nonzero[0].size == 0:\n",
    "        return p\n",
    "    p[nonzero] -= eps * p.size/nonzero[0].size\n",
    "    p += eps\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "class EMForMultivariateBernoulli:\n",
    "\n",
    "    def log_likelihood(self, X, P, pi):\n",
    "\n",
    "        N, D, K = *X.shape, pi.size\n",
    "\n",
    "        log_likelihood = 0\n",
    "        for n in range(N):\n",
    "            x = X[n]\n",
    "            x_likelihood = 0\n",
    "            for k in range(K):\n",
    "                component_joint = pi[k]\n",
    "                for d in range(D):\n",
    "                    component_joint *= (P[k, d]**X[n, d] * (1-P[k, d])**(1-X[n, d]))\n",
    "                x_likelihood += component_joint\n",
    "            log_likelihood += np.log(x_likelihood)\n",
    "\n",
    "        return log_likelihood\n",
    "\n",
    "    def expectation(self, X, parameters, priors):\n",
    "\n",
    "        parameters = 1 - smoothing(1-parameters)\n",
    "        odds = np.divide(parameters, 1-parameters)\n",
    "        responsibilities = list()\n",
    "        for x in X:\n",
    "            likelihoods = np.prod(np.multiply(1-parameters, np.power(odds, x)), axis=1)\n",
    "            joints = np.multiply(priors, likelihoods)\n",
    "            marginal = np.sum(joints)\n",
    "            posteriors = joints / marginal\n",
    "            responsibilities.append(posteriors)\n",
    "        responsibilities = np.array(responsibilities)\n",
    "        \n",
    "        return responsibilities\n",
    "\n",
    "    def maximisation(self, X, responsibilities):\n",
    "\n",
    "        P = np.divide(responsibilities.T @ X, smoothing(np.sum(responsibilities, axis=0)).reshape(-1, 1))\n",
    "        pi = np.mean(responsibilities, axis=0)\n",
    "\n",
    "        return P, pi\n",
    "\n",
    "    def __call__(self, K, X, n_iterations, epsilon):\n",
    "\n",
    "        P = np.random.uniform(size=(K, X.shape[1]))\n",
    "        pi = np.ones(K) / K\n",
    "\n",
    "        log_ls = [self.log_likelihood(X, P, pi)]\n",
    "\n",
    "        for i in tqdm(range(n_iterations)):\n",
    "            responsibilities = self.expectation(X=X, parameters=P, priors=pi)\n",
    "            P, pi = self.maximisation(X, responsibilities=responsibilities)\n",
    "            log_l = self.log_likelihood(X, P=P, pi=pi)\n",
    "            log_ls.append(log_l)\n",
    "            if log_ls[-1] - log_ls[-2] <= epsilon:\n",
    "                break\n",
    "\n",
    "        return P, pi, responsibilities, log_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(p, eps=1e-8):\n",
    "\n",
    "    nonzero = np.nonzero(p)\n",
    "    if nonzero[0].size == 0:\n",
    "        return p\n",
    "    p[nonzero] -= eps * p.size/nonzero[0].size\n",
    "    p += eps\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "class EMForMultivariateBernoulli:\n",
    "\n",
    "    def log_likelihood(self, X, P, pi):\n",
    "\n",
    "        N, D, K = *X.shape, pi.size\n",
    "\n",
    "        log_likelihood = 0\n",
    "        for n in range(N):\n",
    "            x = X[n]\n",
    "            x_likelihood = 0\n",
    "            for k in range(K):\n",
    "                component_joint = pi[k]\n",
    "                for d in range(D):\n",
    "                    component_joint *= (P[k, d]**X[n, d] * (1-P[k, d])**(1-X[n, d]))\n",
    "                x_likelihood += component_joint\n",
    "            log_likelihood += np.log(x_likelihood)\n",
    "\n",
    "        return log_likelihood\n",
    "\n",
    "    def expectation(self, X, parameters, priors):\n",
    "\n",
    "        N, D, K = *X.shape, priors.size\n",
    "\n",
    "        parameters = 1 - smoothing(1-parameters)\n",
    "        responsibilities = np.zeros(shape=(N, K))\n",
    "        for n in range(N):\n",
    "            for k in range(K):\n",
    "                responsibilities[n, k] = priors[k]\n",
    "                for d in range(D):\n",
    "                    responsibilities[n, k] *= (parameters[k, d]**X[n, d] * (1-parameters[k, d])**(1-X[n, d]))\n",
    "            responsibilities[n] /= np.sum(responsibilities[n])\n",
    "        \n",
    "        return responsibilities\n",
    "\n",
    "    def maximisation(self, X, responsibilities):\n",
    "\n",
    "        P = np.divide(responsibilities.T @ X, smoothing(np.sum(responsibilities, axis=0)).reshape(-1, 1))\n",
    "        pi = np.mean(responsibilities, axis=0)\n",
    "\n",
    "        return P, pi\n",
    "\n",
    "    def __call__(self, K, X, n_iterations, epsilon):\n",
    "\n",
    "        P = np.random.uniform(size=(K, X.shape[1]))\n",
    "        pi = np.ones(K) / K\n",
    "\n",
    "        log_ls = [self.log_likelihood(X, P, pi)]\n",
    "\n",
    "        for i in tqdm(range(n_iterations)):\n",
    "            responsibilities = self.expectation(X=X, parameters=P, priors=pi)\n",
    "            P, pi = self.maximisation(X, responsibilities=responsibilities)\n",
    "            log_l = self.log_likelihood(X, P=P, pi=pi)\n",
    "            log_ls.append(log_l)\n",
    "            if log_ls[-1] - log_ls[-2] <= epsilon:\n",
    "                break\n",
    "\n",
    "        return P, pi, responsibilities, log_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1000 [00:00<00:02, 481.38it/s]\n"
     ]
    }
   ],
   "source": [
    "em = EMForMultivariateBernoulli()\n",
    "P, pi, responsibilities, log_ls = em(K=3, X=dataset[:4, 48:53], n_iterations=1000, epsilon=-1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
